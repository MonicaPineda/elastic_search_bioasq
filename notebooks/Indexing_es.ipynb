{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import xml.etree.cElementTree as ET \n",
    "import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import boto3\n",
    "import sys,os,os.path\n",
    "import xml.etree.cElementTree as ET # C implementation of ElementTree\n",
    "import time\n",
    "import gzip\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pubmed_paper():\n",
    "    ''' Used to temporarily store a pubmed paper outside es '''\n",
    "    def __init__(self):\n",
    "        self.pm_id = 0\n",
    "        # every paper has a created_date\n",
    "        self.created_datetime = datetime.datetime.today()\n",
    "        self.title = \"\"\n",
    "        self.abstract = \"\"\n",
    "        self.mesh = \"\"\n",
    "        self.keywords = \"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Pubmed_paper %r>' % (self.pm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pubmed_paper_index(delete=False):    \n",
    "    settings = {\n",
    "        # changing the number of shards after the fact is not \n",
    "        # possible max Gb per shard should be 30Gb, replicas can \n",
    "        # be produced anytime\n",
    "        # https://qbox.io/blog/optimizing-elasticsearch-how-many-shards-per-index\n",
    "        \"number_of_shards\" : 5,\n",
    "        \"number_of_replicas\": 0\n",
    "    }\n",
    "    mappings = {\n",
    "        \"pubmed-paper\": {\n",
    "            \"properties\" : {\n",
    "                \"title\": { \"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"abstract\": { \"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"mesh\": { \"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"keywords\" : { \"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"created_date\": {\n",
    "                    \"type\":   \"date\",\n",
    "                    \"format\": \"yyyy-MM-dd\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if delete:\n",
    "        es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "   # print(\"voy a crear\")\n",
    "    #print(settings)\n",
    "    #print(index_name)\n",
    "    es.indices.create(index=index_name, \n",
    "                      body={ 'settings': settings,\n",
    "                             'mappings': mappings }, \n",
    "                      request_timeout=30)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(single_file, zipped=True):\n",
    "    i = single_file[0]\n",
    "    f = single_file[1]\n",
    "    print(i,f)\n",
    "    print(\"Read file %d filename = %s\" % (i, f))\n",
    "    time0 = time.time()\n",
    "    time1 = time.time()\n",
    "    if zipped:\n",
    "        inF = gzip.open(f, 'rb')\n",
    "    else:\n",
    "        inF = open(f, 'r')\n",
    "    # we have to iterate through the subtrees, ET.parse() would result\n",
    "    # in memory issues\n",
    "    context = ET.iterparse(inF, events=(\"start\", \"end\"))\n",
    "    # turn it into an iterator\n",
    "    context = iter(context)\n",
    "\n",
    "    # get the root element\n",
    "    event, root = context.next()\n",
    "    print(\"Preparing the file: %0.4fsec\" % ((time.time() - time1)))\n",
    "    time1 = time.time()\n",
    "\n",
    "    documents = []\n",
    "    time1 = time.time()\n",
    "    for event, elem in context:\n",
    "        if event == \"end\" and elem.tag == tag['article_tag']:\n",
    "            doc, source = extract_data(elem)\n",
    "            documents.append(doc)\n",
    "            documents.append(source)\n",
    "            elem.clear()\n",
    "    root.clear()\n",
    "    print(\"Extracting the file information: %0.4fsec\" % \n",
    "          ((time.time() - time1)))\n",
    "    time1 = time.time()\n",
    "\n",
    "    res = es.bulk(index=index_name, body=documents, request_timeout=300)\n",
    "    print(\"Indexing data: %0.4fsec\" % ((time.time() - time1)))\n",
    "    print(\"Total time spend on this file: %0.4fsec\\n\" % \n",
    "         ((time.time() - time0)))\n",
    "    processed_files.extend([f])\n",
    "    time.sleep(10)\n",
    "    #os.remove(f) # we directly remove all processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubmedArticle in 2017 Article in 2015\n",
    "def fill_pubmed_papers_table(list_of_files, zipped=True):\n",
    "    # Loop over all files, extract the information and index in bulk\n",
    "    for i, f in enumerate(list_of_files):\n",
    "        process_single_file( (i, f), zipped=True)\n",
    "    #pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    #pool = Pool(processes=2)\n",
    "    #pool.map(process_single_file, [(i,f) for i, f in enumerate(list_of_files)])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify(elem):\n",
    "    from bs4 import BeautifulSoup # just for prettify\n",
    "    '''Return a pretty-printed XML string for the Element.'''\n",
    "    return BeautifulSoup(ET.tostring(elem, 'utf-8'), \"xml\").prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(citation):\n",
    "    new_pubmed_paper = Pubmed_paper()\n",
    "    if citation.tag == 'PubmedArticle':\n",
    "        citation = citation.find(tag['citation_tag'])\n",
    "\n",
    "    new_pubmed_paper.pm_id = citation.find(tag['pmid_tag']).text\n",
    "    new_pubmed_paper.title = citation.find(tag['title_tag']).text\n",
    "    \n",
    "    #Add abstract data\n",
    "    Abstract = citation.find(tag['abstract_tag'])\n",
    "    if Abstract is not None:\n",
    "        # Here we discart information about objectives, design, \n",
    "        # results and conclusion etc.\n",
    "        for text in Abstract.findall(tag['abstract_text_tag']):\n",
    "            if text.text:\n",
    "                if text.get(tag['label_tag']):\n",
    "                    new_pubmed_paper.abstract += '<b>' + text.get(tag['label_tag']) + '</b>: '\n",
    "                new_pubmed_paper.abstract += text.text + '<br>'\n",
    "    \n",
    "    #Add mesh data\n",
    "    MeshHeadingList = citation.find(tag['mesh_list_tag'])\n",
    "    if MeshHeadingList is not None:\n",
    "        # Here we discart information about objectives, design, \n",
    "        # results and conclusion etc.\n",
    "        for text in MeshHeadingList.findall(tag['meshheading_tag']):\n",
    "            description_mesh = text.find( tag['descriptionname_tag'] )\n",
    "            if description_mesh.text:\n",
    "                new_pubmed_paper.mesh += description_mesh.text + ' '\n",
    "    \n",
    "\n",
    "    DateCreated = citation.find(tag['created_date_tag'])\n",
    "    new_pubmed_paper.created_datetime = datetime.datetime(\n",
    "        int(DateCreated.find(tag['created_year_tag']).text),\n",
    "        int(DateCreated.find(tag['created_month_tag']).text),\n",
    "        int(DateCreated.find(tag['created_day_tag']).text)\n",
    "    )\n",
    "    \n",
    "    #Add keywords\n",
    "    KeywordsList = citation.find(tag['keywords_list_tag'])\n",
    "    if KeywordsList is not None:\n",
    "        # Here we discart information about objectives, design, \n",
    "        # results and conclusion etc.\n",
    "        for text in KeywordsList.findall(tag['keyword_tag']):\n",
    "            if text.text:\n",
    "                new_pubmed_paper.keywords += text.text\n",
    "\n",
    "    doc, source = get_es_docs(new_pubmed_paper)\n",
    "    del new_pubmed_paper\n",
    "    return doc, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_docs(paper):\n",
    "    source = {\n",
    "        'title': paper.title,\n",
    "        'created_date': paper.created_datetime.date(),\n",
    "        'abstract': paper.abstract,\n",
    "        'mesh': paper.mesh,\n",
    "        'keywords' : paper.keywords\n",
    "    }\n",
    "    doc = {\n",
    "        \"index\": {\n",
    "            \"_index\": index_name,\n",
    "            \"_type\": type_name,\n",
    "            \"_id\": paper.pm_id\n",
    "        }\n",
    "    }\n",
    "    return doc, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(low_date, up_date, list_of_terms=[]):\n",
    "    term_doc = []\n",
    "    for sub_string in list_of_terms:\n",
    "        term_doc.append({\n",
    "            \"match_phrase\":{\n",
    "                \"title\": sub_string\n",
    "            }\n",
    "        })\n",
    "        term_doc.append({\n",
    "            \"match_phrase\":{\n",
    "                \"abstract\": sub_string\n",
    "            }\n",
    "        })\n",
    "    doc = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [{\n",
    "                    \"range\": {\n",
    "                        \"created_date\":{\n",
    "                            \"gte\" : low_date.strftime('%Y-%m-%d'), \n",
    "                            \"lte\" : up_date.strftime('%Y-%m-%d'), \n",
    "                            \"format\": \"yyyy-MM-dd\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'bool': {\n",
    "                        \"should\": term_doc\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pubmed(baseline_path, create_index=True):\n",
    "    if create_index:\n",
    "        print('Create pubmed paper index: ',index_name)\n",
    "        create_pubmed_paper_index()\n",
    "        print('done')\n",
    "    else:\n",
    "        print('Index already created: ',index_name)\n",
    "    \n",
    "    # fill pubmed papers index\n",
    "    exclude_articles = []\n",
    "    pubmed_folder = baseline_path\n",
    "    # get a list of all .gz files in this folder\n",
    "    list_of_files = [os.path.join(pubmed_folder, f) for f in os.listdir(pubmed_folder) \\\n",
    "                     if os.path.isfile(os.path.join(pubmed_folder, f)) ]\n",
    "    \n",
    "    list_of_files_cleaned = []\n",
    "    for file_s in list_of_files:\n",
    "        if file_s not in exclude_articles:\n",
    "            list_of_files_cleaned += [file_s]\n",
    "        else:\n",
    "            print('removed ',file_s)\n",
    "    \n",
    "    print(\"Indexing \",len(list_of_files_cleaned),' files')\n",
    "    fill_pubmed_papers_table(list_of_files_cleaned, zipped=True)\n",
    "    \n",
    "    with open('index_bioasq2018.txt','w+') as f:\n",
    "        f.write(\",\".join(map(str, list_of_files_cleaned)))\n",
    "    \n",
    "    print('Finish indexing ',len(list_of_files_cleaned),' files.')\n",
    "    print(list_of_files_cleaned)\n",
    "    # pubmed_folder = '/Users/xflorian/Downloads/pubmed_data_2017_daily/'\n",
    "    # get a list of all .gz files in this folder\n",
    "    # list_of_files = [os.path.join(pubmed_folder, f) for f in os.listdir(pubmed_folder) \\ \n",
    "    #                 if os.path.isfile(os.path.join(pubmed_folder, f)) and f[-2:] == 'gz']\n",
    "    # fill_pubmed_papers_table(list_of_files)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = {\n",
    "    'article_tag': 'PubmedArticle',\n",
    "    'citation_tag': 'MedlineCitation',\n",
    "    'pmid_tag': 'PMID',\n",
    "    'title_tag': 'Article/ArticleTitle',\n",
    "    'abstract_tag': 'Article/Abstract', \n",
    "    'abstract_text_tag': 'AbstractText', \n",
    "    'label_tag': 'Label', \n",
    "    'created_date_tag': 'DateRevised',\n",
    "    'created_year_tag': 'Year',\n",
    "    'created_month_tag': 'Month',\n",
    "    'created_day_tag': 'Day',\n",
    "    'mesh_list_tag': 'MeshHeadingList',\n",
    "    'meshheading_tag': 'MeshHeading',\n",
    "    'descriptionname_tag':  'DescriptorName',\n",
    "    'keywords_list_tag' : 'KeywordList',\n",
    "    'keyword_tag' : 'Keyword'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=['localhost:9200'])\n",
    "data_year = '2020'\n",
    "baseline_path = #add data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = data_year+'test'\n",
    "es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "type_name = 'pubmed-paper'\n",
    "process_pubmed(baseline_path, create_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\" : {\n",
    "              \"query\":    \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\", \n",
    "              \"fields\": [ \"abstract\", \"title\", \"mesh\" ] \n",
    "            }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.search(index=index_name, body=query, request_timeout=30) \n",
    "for doc in res['hits']['hits']:\n",
    "    #print(\"%s) %s\" % (doc['_id'], doc['_source']))\n",
    "    print(doc['_id'],doc['_source']['title'],doc['_source']['mesh'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1597596168720"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}